# Llama.cpp server base URL (OpenAI-compatible)
LLAMA_CPP_BASE_URL=http://127.0.0.1:8080
# Optional model routing map (model=url, comma-separated).
# Example:
# MODEL_UPSTREAMS=llama-8b=http://127.0.0.1:8081,llama-70b-q4=http://127.0.0.1:8082,llama-70b-q5=http://127.0.0.1:8083
MODEL_UPSTREAMS=
# Local model file map for multi-server launcher (model=path, comma-separated).
# Example:
# MODEL_PATHS=llama-8b=./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf,llama-70b-q4=./models/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf,llama-70b-q5=./models/Meta-Llama-3.1-70B-Instruct-Q5_K_M/Meta-Llama-3.1-70B-Instruct-Q5_K_M-00001-of-00002.gguf
MODEL_PATHS=
# Optional subset to start (comma-separated). Empty = start all from MODEL_UPSTREAMS.
ENABLED_MODELS=llama-8b

# API keys for this proxy:
# - OPENAI_API_KEY: single fallback key
# - OPENAI_API_KEYS: multiple keys, optionally with client IDs
#   format: keyA:clientA,keyB:clientB,keyC
OPENAI_API_KEY=local
OPENAI_API_KEYS=

# Security
CORS_ALLOWED_ORIGINS=http://localhost:3000
RATE_LIMIT_RPM=120

# Logging level: debug, info, warning, error
LOG_LEVEL=info

# Retry policy
MAX_RETRIES=2
RETRY_BACKOFF_S=0.35

# Network timeouts
CONNECT_TIMEOUT_S=5
TIMEOUT_CHAT_S=120
TIMEOUT_EMBEDDINGS_S=60
TIMEOUT_COMPLETIONS_S=120
TIMEOUT_MODELS_S=10

# API server settings (uvicorn)
API_HOST=127.0.0.1
API_PORT=8000

# llama.cpp server launcher settings (optional)
LLAMA_CPP_BIN=./llama.cpp/build/bin/llama-server
LLAMA_MODEL=./models/llama.gguf
LLAMA_PORT=8080
LLAMA_HOST=127.0.0.1
LLAMA_CTX=4096
LLAMA_N_BATCH=512
LLAMA_THREADS=0
LLAMA_THREADS_BATCH=0
LLAMA_EMBEDDINGS=1
