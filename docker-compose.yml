version: "3.9"

services:
  proxy:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llama_proxy
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY:-local}
      OPENAI_API_KEYS: ${OPENAI_API_KEYS:-}
      CORS_ALLOWED_ORIGINS: ${CORS_ALLOWED_ORIGINS:-http://localhost:3000}
      RATE_LIMIT_RPM: ${RATE_LIMIT_RPM:-120}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      MAX_RETRIES: ${MAX_RETRIES:-2}
      RETRY_BACKOFF_S: ${RETRY_BACKOFF_S:-0.35}
      CONNECT_TIMEOUT_S: ${CONNECT_TIMEOUT_S:-5}
      TIMEOUT_CHAT_S: ${TIMEOUT_CHAT_S:-120}
      TIMEOUT_EMBEDDINGS_S: ${TIMEOUT_EMBEDDINGS_S:-60}
      TIMEOUT_COMPLETIONS_S: ${TIMEOUT_COMPLETIONS_S:-120}
      TIMEOUT_MODELS_S: ${TIMEOUT_MODELS_S:-10}
      MODEL_UPSTREAMS: llama-8b=http://llama_8b:8080,llama-70b-q4=http://llama_70b_q4:8080,llama-70b-q5=http://llama_70b_q5:8080
    depends_on:
      - llama_8b
    networks:
      - llama_net

  llama_8b:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llama_8b
    restart: unless-stopped
    command:
      - -m
      - /models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --ctx-size
      - "4096"
      - --batch-size
      - "1024"
      - --threads
      - "20"
      - --threads-batch
      - "20"
      - --embeddings
    volumes:
      - ./models:/models:ro
    networks:
      - llama_net

  llama_70b_q4:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llama_70b_q4
    restart: unless-stopped
    profiles: ["full"]
    command:
      - -m
      - /models/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --ctx-size
      - "4096"
      - --batch-size
      - "1024"
      - --threads
      - "20"
      - --threads-batch
      - "20"
      - --embeddings
    volumes:
      - ./models:/models:ro
    networks:
      - llama_net

  llama_70b_q5:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llama_70b_q5
    restart: unless-stopped
    profiles: ["full"]
    command:
      - -m
      - /models/Meta-Llama-3.1-70B-Instruct-Q5_K_M/Meta-Llama-3.1-70B-Instruct-Q5_K_M-00001-of-00002.gguf
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --ctx-size
      - "4096"
      - --batch-size
      - "1024"
      - --threads
      - "20"
      - --threads-batch
      - "20"
      - --embeddings
    volumes:
      - ./models:/models:ro
    networks:
      - llama_net

networks:
  llama_net:
    name: llama_net
