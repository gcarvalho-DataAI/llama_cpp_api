# llama.cpp OpenAI-Compatible API

Production-grade FastAPI proxy for local LLM serving on `llama.cpp`, exposing OpenAI-compatible endpoints for chat, completions, and embeddings.

## Why this project matters
This repository demonstrates practical AI backend engineering for real deployment scenarios, not just demos.

- Runs local LLM inference with OpenAI API compatibility (`/v1/*`)
- Adds reliability controls expected in production (timeouts, retries, rate limits)
- Improves operability with metrics, structured logs, and traceable request IDs
- Supports secure multi-client usage through API keys and per-client throttling
- Keeps integration simple for existing OpenAI-based applications

## LLM Engineer Snapshot
- Domain: LLM serving, inference reliability, and production API design
- Focus: latency/throughput tradeoffs, model runtime constraints, and endpoint compatibility
- Stack: Python, FastAPI, Pydantic, httpx, llama.cpp, GGUF models, Prometheus-style metrics
- Value: enables cost-efficient local inference while preserving OpenAI-compatible integration

## What this demonstrates for LLM Engineer roles
- Inference serving architecture: wraps `llama.cpp` with a hardened API layer for real client workloads
- API contract engineering: keeps OpenAI request/response patterns for easy migration and interoperability
- Runtime reliability: retries, backoff, timeouts, and rate limits to handle unstable upstream behavior
- Observability for model serving: request IDs, structured logs, and metrics for incident triage and tuning
- Performance-oriented operations: CPU-first tuning knobs (`threads`, `batch`, `ctx`) and reproducible load checks

## Architecture

```text
OpenAI-compatible client/app
          |
          v
   FastAPI Proxy (this repo)
   - auth & per-client limits
   - schema validation
   - retries/timeouts
   - metrics/logging
          |
          v
   llama.cpp server (local)
          |
          v
      GGUF model
```

## Endpoints
- `GET /health`
- `GET /metrics` (Prometheus text format)
- `GET /v1/models`
- `POST /v1/chat/completions`
- `POST /v1/completions`
- `POST /v1/embeddings`

## Production features implemented
- OpenAI-style schema validation with Pydantic
- Streaming passthrough for `stream=true` (SSE)
- Retry with exponential backoff for transient upstream failures
- Route-specific timeouts
- API key auth with per-client keys (`OPENAI_API_KEYS`)
- Sliding-window rate limit per client
- Structured JSON logs with `x-request-id`
- CORS allowlist
- Smoke tests (`pytest`) and load test script

## Prerequisites
- Python 3.10+
- Built `llama.cpp` server binary (`llama-server`)
- GGUF model file in `./models`

## Quick start

1. Create virtualenv and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. Configure:

```bash
cp .env.example .env
```

If you want model-aware routing from request payload (`"model": "..."`), configure multiple upstreams:

```env
MODEL_UPSTREAMS=llama-8b=http://127.0.0.1:8081,llama-70b-q4=http://127.0.0.1:8082,llama-70b-q5=http://127.0.0.1:8083
```

In this mode, the proxy routes each request to the upstream selected by the payload `model`.
Each `llama-server` process should load one model.

Also define local model files for the multi-launcher script:

```env
MODEL_PATHS=llama-8b=./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf,llama-70b-q4=./models/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf,llama-70b-q5=./models/Meta-Llama-3.1-70B-Instruct-Q5_K_M/Meta-Llama-3.1-70B-Instruct-Q5_K_M-00001-of-00002.gguf
ENABLED_MODELS=llama-8b
```

`ENABLED_MODELS` lets you start only a subset of models (recommended on limited RAM).

3. Start `llama.cpp`:

```bash
./scripts/run_llama_server.sh
```

For multi-model mode, start multiple backends:

```bash
./scripts/run_llama_servers_multi.sh
```

4. Start proxy API:

```bash
./scripts/run_api.sh
```

## Security and auth config
Use one key:

```env
OPENAI_API_KEY=local
```

Use multiple keys with client IDs:

```env
OPENAI_API_KEYS=key_a:team_a,key_b:team_b,key_c
```

`key_c` gets an autogenerated client id.

## Example requests

Chat completion:

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama",
    "messages": [{"role": "user", "content": "hello"}],
    "temperature": 0.2
  }'
```

Streaming chat:

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -N \
  -d '{
    "model": "llama",
    "messages": [{"role": "user", "content": "write one short poem"}],
    "stream": true
  }'
```

Embeddings:

```bash
curl http://127.0.0.1:8000/v1/embeddings \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama",
    "input": ["hello world"]
  }'
```

## CPU tuning (i9 14900K baseline)
Suggested baseline in `.env`:

```env
LLAMA_THREADS=20
LLAMA_THREADS_BATCH=20
LLAMA_CTX=4096
LLAMA_N_BATCH=1024
LLAMA_EMBEDDINGS=1
```

Recommended quantization starting points:
- `8B Q5_K_M` for quality/latency balance
- `8B Q4_K_M` for lower latency

## Validation and tests
Run smoke tests:

```bash
source .venv/bin/activate
pytest -q
```

Run basic load test:

```bash
source .venv/bin/activate
python scripts/load_test.py --requests 40 --concurrency 8 --model llama --api-key local
```

## Integration with OpenAI-based apps
Point your existing app to this proxy:

```env
OPENAI_ENDPOINT=http://127.0.0.1:8000/v1
OPENAI_API_KEY=local
OPENAI_CHAT_MODEL=llama
```

For multi-model mode, set `OPENAI_CHAT_MODEL` dynamically in your client payload and make sure it matches keys in `MODEL_UPSTREAMS`.

To stop multi-backend servers:

```bash
./scripts/stop_llama_servers_multi.sh
```

## Suggested GitHub "About" text
LLM Engineer portfolio project: production-ready OpenAI-compatible API on top of llama.cpp, with streaming, validation, retries, rate limiting, and observability.
