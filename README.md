# llama.cpp OpenAI-compatible API (CPU-focused)

FastAPI proxy for a local `llama.cpp` server with OpenAI-compatible endpoints.

## Endpoints
- `GET /health`
- `GET /metrics` (Prometheus text format)
- `GET /v1/models`
- `POST /v1/chat/completions`
- `POST /v1/completions`
- `POST /v1/embeddings`

## Production features implemented
- OpenAI-style schema validation with Pydantic
- Streaming passthrough for `stream=true` (SSE)
- Retry with exponential backoff for transient upstream failures
- Route-specific timeouts
- API key auth with per-client keys (`OPENAI_API_KEYS`)
- Sliding-window rate limit per client
- Structured JSON logs with `x-request-id`
- CORS allowlist
- Smoke tests (`pytest`) and simple load test script

## Prerequisites
- Python 3.10+
- Built `llama.cpp` server binary (`llama-server`)
- GGUF model file in `./models`

## Quick start

1. Create virtualenv and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. Configure:

```bash
cp .env.example .env
```

3. Start `llama.cpp`:

```bash
./scripts/run_llama_server.sh
```

4. Start proxy API:

```bash
./scripts/run_api.sh
```

## Security and auth config
Use one key:

```env
OPENAI_API_KEY=local
```

Use multiple keys with client IDs:

```env
OPENAI_API_KEYS=key_a:team_a,key_b:team_b,key_c
```

`key_c` gets an autogenerated client id.

## Example requests

Chat completion:

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama",
    "messages": [{"role": "user", "content": "hello"}],
    "temperature": 0.2
  }'
```

Streaming chat:

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -N \
  -d '{
    "model": "llama",
    "messages": [{"role": "user", "content": "write one short poem"}],
    "stream": true
  }'
```

Embeddings:

```bash
curl http://127.0.0.1:8000/v1/embeddings \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama",
    "input": ["hello world"]
  }'
```

## CPU tuning (i9 14900K)
Suggested baseline in `.env`:

```env
LLAMA_THREADS=20
LLAMA_THREADS_BATCH=20
LLAMA_CTX=4096
LLAMA_N_BATCH=1024
LLAMA_EMBEDDINGS=1
```

Recommended quantization starting points:
- `8B Q5_K_M` for quality/latency balance
- `8B Q4_K_M` for lower latency

## Validation and tests
Run smoke tests:

```bash
source .venv/bin/activate
pytest -q
```

Run simple load test:

```bash
source .venv/bin/activate
python scripts/load_test.py --requests 40 --concurrency 8 --model llama --api-key local
```

## Integration with your existing project
In your other app (OpenAI-compatible client), point to this proxy:

```env
OPENAI_ENDPOINT=http://127.0.0.1:8000/v1
OPENAI_API_KEY=local
OPENAI_CHAT_MODEL=llama
```
