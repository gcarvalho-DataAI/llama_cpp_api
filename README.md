# llama.cpp OpenAI-Compatible API

Production-grade FastAPI proxy for local LLM serving on `llama.cpp`, exposing OpenAI-compatible endpoints for chat, completions, and embeddings.

## Why this project matters
This repository demonstrates practical AI backend engineering for real deployment scenarios, not just demos.

- Runs local LLM inference with OpenAI API compatibility (`/v1/*`)
- Adds reliability controls expected in production (timeouts, retries, rate limits)
- Improves operability with metrics, structured logs, and traceable request IDs
- Supports secure multi-client usage through API keys and per-client throttling
- Keeps integration simple for existing OpenAI-based applications

## Recruiter Snapshot
- Domain: LLM infrastructure, AI backend systems, API platform engineering
- Focus: performance, resilience, observability, and integration ergonomics
- Stack: Python, FastAPI, Pydantic, httpx, llama.cpp, Prometheus-style metrics
- Value: lowers inference cost by enabling local CPU-first serving with standard API contracts

## Architecture

```text
OpenAI-compatible client/app
          |
          v
   FastAPI Proxy (this repo)
   - auth & per-client limits
   - schema validation
   - retries/timeouts
   - metrics/logging
          |
          v
   llama.cpp server (local)
          |
          v
      GGUF model
```

## Endpoints
- `GET /health`
- `GET /metrics` (Prometheus text format)
- `GET /v1/models`
- `POST /v1/chat/completions`
- `POST /v1/completions`
- `POST /v1/embeddings`

## Production features implemented
- OpenAI-style schema validation with Pydantic
- Streaming passthrough for `stream=true` (SSE)
- Retry with exponential backoff for transient upstream failures
- Route-specific timeouts
- API key auth with per-client keys (`OPENAI_API_KEYS`)
- Sliding-window rate limit per client
- Structured JSON logs with `x-request-id`
- CORS allowlist
- Smoke tests (`pytest`) and load test script

## Prerequisites
- Python 3.10+
- Built `llama.cpp` server binary (`llama-server`)
- GGUF model file in `./models`

## Quick start

1. Create virtualenv and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. Configure:

```bash
cp .env.example .env
```

3. Start `llama.cpp`:

```bash
./scripts/run_llama_server.sh
```

4. Start proxy API:

```bash
./scripts/run_api.sh
```

## Security and auth config
Use one key:

```env
OPENAI_API_KEY=local
```

Use multiple keys with client IDs:

```env
OPENAI_API_KEYS=key_a:team_a,key_b:team_b,key_c
```

`key_c` gets an autogenerated client id.

## Example requests

Chat completion:

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama",
    "messages": [{"role": "user", "content": "hello"}],
    "temperature": 0.2
  }'
```

Streaming chat:

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -N \
  -d '{
    "model": "llama",
    "messages": [{"role": "user", "content": "write one short poem"}],
    "stream": true
  }'
```

Embeddings:

```bash
curl http://127.0.0.1:8000/v1/embeddings \
  -H "Authorization: Bearer local" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama",
    "input": ["hello world"]
  }'
```

## CPU tuning (i9 14900K baseline)
Suggested baseline in `.env`:

```env
LLAMA_THREADS=20
LLAMA_THREADS_BATCH=20
LLAMA_CTX=4096
LLAMA_N_BATCH=1024
LLAMA_EMBEDDINGS=1
```

Recommended quantization starting points:
- `8B Q5_K_M` for quality/latency balance
- `8B Q4_K_M` for lower latency

## Validation and tests
Run smoke tests:

```bash
source .venv/bin/activate
pytest -q
```

Run basic load test:

```bash
source .venv/bin/activate
python scripts/load_test.py --requests 40 --concurrency 8 --model llama --api-key local
```

## Integration with OpenAI-based apps
Point your existing app to this proxy:

```env
OPENAI_ENDPOINT=http://127.0.0.1:8000/v1
OPENAI_API_KEY=local
OPENAI_CHAT_MODEL=llama
```

## Suggested GitHub "About" text
Production-grade local LLM API: OpenAI-compatible FastAPI proxy for llama.cpp with retries, rate limiting, metrics, streaming, and secure multi-client support.
